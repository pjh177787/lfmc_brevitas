{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os, time\n",
    "gpu = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine has 1 GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Machine has %d GPU'%torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"/workspace/dataset/GOLD_XYZ_OSC.0001_1024.hdf5\"\n",
    "os.path.isfile(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2555904, 1024, 2)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data loader\n",
    "batch_size = 1024\n",
    "\n",
    "class radioml_18_dataset(Dataset):\n",
    "    def __init__(self, dataset_path):\n",
    "        super(radioml_18_dataset, self).__init__()\n",
    "        h5_file = h5py.File(dataset_path,'r')\n",
    "        self.data = h5_file['X']\n",
    "        print(self.data.shape)\n",
    "        self.mod = np.argmax(h5_file['Y'], axis=1) # comes in one-hot encoding\n",
    "        self.snr = h5_file['Z'][:,0]\n",
    "        self.len = self.data.shape[0]\n",
    "\n",
    "        self.mod_classes = ['OOK','4ASK','8ASK','BPSK','QPSK','8PSK','16PSK','32PSK',\n",
    "        '16APSK','32APSK','64APSK','128APSK','16QAM','32QAM','64QAM','128QAM','256QAM',\n",
    "        'AM-SSB-WC','AM-SSB-SC','AM-DSB-WC','AM-DSB-SC','FM','GMSK','OQPSK']\n",
    "        self.snr_classes = np.arange(-20., 32., 2) # -20dB to 30dB\n",
    "\n",
    "        # do not touch this seed to ensure the prescribed train/test split!\n",
    "        np.random.seed(2018)\n",
    "        train_indices = []\n",
    "        test_indices = []\n",
    "        for mod in range(0, 24): # all modulations (0 to 23)\n",
    "            for snr_idx in range(0, 26): # all SNRs (0 to 25 = -20dB to +30dB)\n",
    "                # 'X' holds frames strictly ordered by modulation and SNR\n",
    "                start_idx = 26*4096*mod + 4096*snr_idx\n",
    "                indices_subclass = list(range(start_idx, start_idx+4096))\n",
    "                \n",
    "                # 90%/10% training/test split, applied evenly for each mod-SNR pair\n",
    "                split = int(np.ceil(0.1 * 4096)) \n",
    "                np.random.shuffle(indices_subclass)\n",
    "                train_indices_subclass = indices_subclass[split:]\n",
    "                test_indices_subclass = indices_subclass[:split]\n",
    "                \n",
    "                # you could train on a subset of the data, e.g. based on the SNR\n",
    "                # here we use all available training samples\n",
    "                if snr_idx >= 0:\n",
    "                    train_indices.extend(train_indices_subclass)\n",
    "                test_indices.extend(test_indices_subclass)\n",
    "                \n",
    "        self.train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "        self.test_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # transpose frame into Pytorch channels-first format (NCL = -1,2,1024)\n",
    "        return self.data[idx].transpose(), self.mod[idx], self.snr[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = radioml_18_dataset(dataset_path)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, sampler=dataset.train_sampler)\n",
    "test_dataloader = DataLoader(dataset, batch_size=batch_size, sampler=dataset.test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import IntBias\n",
    "from brevitas.inject.enum import ScalingImplType\n",
    "from brevitas.inject.defaults import Int8ActPerTensorFloatMinMaxInit\n",
    "\n",
    "\n",
    "class InputQuantizer(Int8ActPerTensorFloatMinMaxInit):\n",
    "    bit_width = 8\n",
    "    min_val = -2.0\n",
    "    max_val = 2.0\n",
    "    scaling_impl_type = ScalingImplType.CONST # Fix the quantization range to [min_val, max_val]\n",
    "\n",
    "class IlliNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        a_bits = 4\n",
    "        w_bits = 4\n",
    "        self.input = qnn.QuantHardTanh(act_quant=InputQuantizer)\n",
    "        \n",
    "        self.conv0 = qnn.QuantConv1d(2, 16, 3, padding=1, weight_bit_width=w_bits, bias=False)\n",
    "        self.bn0 = nn.BatchNorm1d(16)\n",
    "\n",
    "        self.conv1 = qnn.QuantConv1d(16, 32, 3, padding=1, weight_bit_width=w_bits, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = qnn.QuantConv1d(32, 64, 3, padding=1, weight_bit_width=w_bits, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = qnn.QuantConv1d(64, 64, 3, padding=1, weight_bit_width=w_bits, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv4 = qnn.QuantConv1d(64, 64, 3, padding=1, weight_bit_width=w_bits, bias=False)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv5 = qnn.QuantConv1d(64, 64, 3, padding=1, weight_bit_width=w_bits, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv6 = qnn.QuantConv1d(64, 64, 3, padding=1, weight_bit_width=w_bits, bias=False)\n",
    "        self.bn6 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv7 = qnn.QuantConv1d(64, 64, 3, padding=1, weight_bit_width=w_bits, bias=False)\n",
    "        self.bn7 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.fc1 = qnn.QuantLinear(256, 64, weight_bit_width=w_bits, bias=False)\n",
    "        self.bn_dense = nn.BatchNorm1d(64)\n",
    "        self.fc2 = qnn.QuantLinear(64, 24, weight_bit_width=w_bits, bias=False)\n",
    "\n",
    "        self.relu = qnn.QuantReLU(bit_width=a_bits)\n",
    "        self.pooling = nn.MaxPool1d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "\n",
    "        x = self.conv0(x)\n",
    "        x = self.bn0(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling(x)\n",
    "        \n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling(x)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.relu(self.bn_dense(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = IlliNet()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print('Muti-GPU activated')\n",
    "    net = nn.DataParallel(net)\n",
    "\n",
    "save_path = './IlliNet_trained.pth'\n",
    "if os.path.isfile(save_path):\n",
    "    saved_state = torch.load(save_path)\n",
    "    net.load_state_dict(saved_state, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First evaluate accuracy of the existing model\n",
    "total = 0\n",
    "correct = 0\n",
    "global_accu = []\n",
    "for i, data in enumerate(data_loader_test, 0):\n",
    "    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    outputs = net(inputs)\n",
    "    labels = labels.type(torch.long)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "global_accu.append(100 * correct / total)\n",
    "# print('\\n[Epoch %d] loss: %f'%(epoch + 1, running_loss))\n",
    "print('Accuracy of the network: %f %%'%(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.849476680132206\n",
      "Layer weight sparsity values: \n",
      "Conv 0: 0.25\n",
      "Conv 1: 0.3821614583333333\n",
      "Conv 2: 0.4210611979166667\n",
      "Conv 3: 0.5135904947916666\n",
      "Conv 4: 0.593505859375\n",
      "Conv 5: 0.53271484375\n",
      "Conv 6: 0.536865234375\n",
      "Conv 7: 0.5533040364583334\n",
      "FC 1: 0.6407470703125\n",
      "FC 2: 0.9264322916666666\n"
     ]
    }
   ],
   "source": [
    "def global_sparsity_eval():\n",
    "      return 100. * float(torch.sum(net.conv0.weight == 0) \n",
    "                          + torch.sum(net.conv1.weight == 0) \n",
    "                          + torch.sum(net.conv2.weight == 0) \n",
    "                          + torch.sum(net.conv3.weight == 0) \n",
    "                          + torch.sum(net.conv4.weight == 0)\n",
    "                          + torch.sum(net.conv5.weight == 0) \n",
    "                          + torch.sum(net.conv6.weight == 0) \n",
    "                          + torch.sum(net.conv7.weight == 0) \n",
    "                          + torch.sum(net.fc1.weight == 0) \n",
    "                          + torch.sum(net.fc2.weight == 0)) / float(net.conv0.weight.nelement() \n",
    "                            + net.conv1.weight.nelement() \n",
    "                            + net.conv2.weight.nelement() \n",
    "                            + net.conv3.weight.nelement() \n",
    "                            + net.conv4.weight.nelement() \n",
    "                            + net.conv5.weight.nelement() \n",
    "                            + net.conv6.weight.nelement() \n",
    "                            + net.conv7.weight.nelement()\n",
    "                            + net.fc1.weight.nelement() \n",
    "                            + net.fc2.weight.nelement())\n",
    "def layer_sparsity_eval():\n",
    "    print('Layer weight sparsity values: ')\n",
    "    print('Conv 0:', torch.sum(net.conv0.weight == 0).item()/net.conv0.weight.nelement())\n",
    "    print('Conv 1:', torch.sum(net.conv1.weight == 0).item()/net.conv1.weight.nelement())\n",
    "    print('Conv 2:', torch.sum(net.conv2.weight == 0).item()/net.conv2.weight.nelement())\n",
    "    print('Conv 3:', torch.sum(net.conv3.weight == 0).item()/net.conv3.weight.nelement())\n",
    "    print('Conv 4:', torch.sum(net.conv4.weight == 0).item()/net.conv4.weight.nelement())\n",
    "    print('Conv 5:', torch.sum(net.conv5.weight == 0).item()/net.conv5.weight.nelement())\n",
    "    print('Conv 6:', torch.sum(net.conv6.weight == 0).item()/net.conv6.weight.nelement())\n",
    "    print('Conv 7:', torch.sum(net.conv7.weight == 0).item()/net.conv7.weight.nelement())\n",
    "    print('FC 1:', torch.sum(net.fc1.weight == 0).item()/net.fc1.weight.nelement())\n",
    "    print('FC 2:', torch.sum(net.fc2.weight == 0).item()/net.fc2.weight.nelement())\n",
    "\n",
    "init_global_sparsity = global_sparsity_eval()\n",
    "print(init_global_sparsity)\n",
    "layer_sparsity_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2021-10-09 22:17:18.028140\n",
      "date and time = 211009_2217\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_str = now.strftime(\"%y%m%d_%H%M\")\n",
    "print(\"date and time =\", dt_str)\n",
    "\n",
    "# Adjustable hyperparameters\n",
    "model_name = 'IlliNet'\n",
    "\n",
    "model_save_path = 'saves/%s_%s.pth'%(model_name, dt_str)\n",
    "model_save_path_best = 'saves/%s_%s_best.pth'%(model_name, dt_str)\n",
    "model_save_path_final = 'saves/%s_%s_final.pth'%(model_name, dt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):    \n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "        \n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            labels = labels.type(torch.long)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return 100*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model sparsity: 55.849477 %\n",
      "Sparsity after pruning: 55.849477 %\n",
      "Step 2247/2247 | Loss = 1.3795, Time = 0.049525\n",
      "[Epoch 1] loss: 2811.759611\n",
      "Training accuracy of the network: 56.821680 %\n",
      "Testing accuracy of the network: 57.132583 %\n",
      "Model saved at  saves/IlliNet_211009_2217.pth\n",
      "After epoch sparsity: 55.849477 %\n",
      "Layer weight sparsity values: \n",
      "Conv 0: 0.25\n",
      "Conv 1: 0.3821614583333333\n",
      "Conv 2: 0.4210611979166667\n",
      "Conv 3: 0.5135904947916666\n",
      "Conv 4: 0.593505859375\n",
      "Conv 5: 0.53271484375\n",
      "Conv 6: 0.536865234375\n",
      "Conv 7: 0.5533040364583334\n",
      "FC 1: 0.6407470703125\n",
      "FC 2: 0.9264322916666666\n",
      "Epoch Time: 635.5432 \n",
      "\n",
      "Sparsity after pruning: 55.952763 %\n",
      "Step 2247/2247 | Loss = 1.1959, Time = 0.030650\n",
      "[Epoch 2] loss: 2823.689125\n",
      "Training accuracy of the network: 56.611773 %\n",
      "Testing accuracy of the network: 56.603737 %\n",
      "Locking this epoch to increase accuracy\n",
      "Best model saved at  saves/IlliNet_211009_2217_best.pth\n",
      "After epoch sparsity: 55.952763 %\n",
      "Layer weight sparsity values: \n",
      "Conv 0: 0.25\n",
      "Conv 1: 0.3821614583333333\n",
      "Conv 2: 0.4210611979166667\n",
      "Conv 3: 0.5135904947916666\n",
      "Conv 4: 0.593505859375\n",
      "Conv 5: 0.53271484375\n",
      "Conv 6: 0.536865234375\n",
      "Conv 7: 0.5533040364583334\n",
      "FC 1: 0.64617919921875\n",
      "FC 2: 0.9270833333333334\n",
      "Epoch Time: 701.9719 \n",
      "\n",
      "Sparsity after pruning: 55.952763 %\n",
      "Step 2247/2247 | Loss = 1.0600, Time = 0.071247\n",
      "[Epoch 3] loss: 2823.740706\n",
      "Training accuracy of the network: 56.610033 %\n",
      "Testing accuracy of the network: 56.509537 %\n",
      "Locking this epoch to increase accuracy\n",
      "Model saved at  saves/IlliNet_211009_2217.pth\n",
      "After epoch sparsity: 55.952763 %\n",
      "Layer weight sparsity values: \n",
      "Conv 0: 0.25\n",
      "Conv 1: 0.3821614583333333\n",
      "Conv 2: 0.4210611979166667\n",
      "Conv 3: 0.5135904947916666\n",
      "Conv 4: 0.593505859375\n",
      "Conv 5: 0.53271484375\n",
      "Conv 6: 0.536865234375\n",
      "Conv 7: 0.5533040364583334\n",
      "FC 1: 0.64617919921875\n",
      "FC 2: 0.9270833333333334\n",
      "Epoch Time: 726.1441 \n",
      "\n",
      "Finished Training, elapsed 2063.669 seconds\n"
     ]
    }
   ],
   "source": [
    "global_loss = []\n",
    "global_accu = []\n",
    "num_epochs = 3\n",
    "\n",
    "step = 0.015\n",
    "sparsity = 0.0\n",
    "spars_eval = 0.0\n",
    "\n",
    "parameters_fc_only = (\n",
    "    (net.fc1, 'weight'),\n",
    "    (net.fc2, 'weight'),\n",
    ")\n",
    "\n",
    "parameters_all = (\n",
    "    (net.conv0, 'weight'),\n",
    "    (net.conv1, 'weight'),\n",
    "    (net.conv2, 'weight'),\n",
    "    (net.conv3, 'weight'),\n",
    "    (net.conv4, 'weight'),\n",
    "    (net.conv5, 'weight'),\n",
    "    (net.conv6, 'weight'),\n",
    "    (net.conv7, 'weight'),\n",
    "    (net.fc1, 'weight'),\n",
    "    (net.fc2, 'weight'),\n",
    ")\n",
    "\n",
    "\n",
    "best_acc = 0\n",
    "ts = time.time()\n",
    "net.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_steps = len(train_dataloader)\n",
    "    \n",
    "    t2 = time.time()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        if epoch == 0:\n",
    "            sparsity = global_sparsity_eval()/100\n",
    "            print('Initialized model sparsity: %f %%'%(100*sparsity))\n",
    "        else:\n",
    "            sparsity = step\n",
    "        prune.global_unstructured(parameters_all, pruning_method = prune.L1Unstructured, amount=sparsity)\n",
    "    else:\n",
    "        sparsity = step\n",
    "        prune.global_unstructured(parameters_fc_only, pruning_method = prune.L1Unstructured, amount=sparsity)\n",
    "        \n",
    "    spars_eval = global_sparsity_eval()\n",
    "    print('Sparsity after pruning: %f %%'%(spars_eval))\n",
    "    \n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "        outputs = net(inputs)\n",
    "        labels = labels.type(torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        t1 = time.time()\n",
    "        if i%1 == 0 or i == num_steps:\n",
    "            print('\\rStep %d/%d | Loss = %.4f, Time = %.6f'%(i+1, num_steps, loss, t1-t0), end='')\n",
    "        \n",
    "    training_acc = 100 * correct / total\n",
    "    global_loss.append(running_loss)\n",
    "    global_accu.append(training_acc)\n",
    "    print('\\n[Epoch %d] loss: %f'%(epoch + 1, running_loss))\n",
    "    print('Training accuracy of the network: %f %%'%(training_acc))\n",
    "    \n",
    "    step = 0.015\n",
    "    testing_acc = test(net, test_dataloader)\n",
    "    print('Testing accuracy of the network: %f %%'%(testing_acc))\n",
    "    \n",
    "    if testing_acc < 57:\n",
    "        step = 0\n",
    "        print(\"Locking this epoch to increase accuracy\")\n",
    "        \n",
    "    if testing_acc > best_acc and epoch != 0:\n",
    "        print(\"Best model saved at \", model_save_path_best)\n",
    "        torch.save(net.state_dict(), model_save_path_best)\n",
    "        best_acc = testing_acc\n",
    "    else:\n",
    "        print(\"Model saved at \", model_save_path)\n",
    "        torch.save(net.state_dict(), model_save_path)\n",
    "        \n",
    "    print('After epoch sparsity: %f %%'%(global_sparsity_eval()))\n",
    "    layer_sparsity_eval()\n",
    "    t3 = time.time()\n",
    "    print('Epoch Time: %.4f '%(t3 - t2))\n",
    "    print()\n",
    "tt = time.time()\n",
    "print('Finished Training, elapsed %.3f seconds'%(tt-ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantLinear(\n",
       "  in_features=64, out_features=24, bias=False\n",
       "  (input_quant): ActQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "  )\n",
       "  (output_quant): ActQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "  )\n",
       "  (weight_quant): WeightQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "    (tensor_quant): RescalingIntQuant(\n",
       "      (int_quant): IntQuant(\n",
       "        (float_to_int_impl): RoundSte()\n",
       "        (tensor_clamp_impl): TensorClampSte()\n",
       "        (delay_wrapper): DelayWrapper(\n",
       "          (delay_impl): _NoDelay()\n",
       "        )\n",
       "      )\n",
       "      (scaling_impl): StatsFromParameterScaling(\n",
       "        (parameter_list_stats): _ParameterListStats(\n",
       "          (first_tracked_param): _ViewParameterWrapper(\n",
       "            (view_shape_impl): OverTensorView()\n",
       "          )\n",
       "          (stats): _Stats(\n",
       "            (stats_impl): AbsMax()\n",
       "          )\n",
       "        )\n",
       "        (stats_scaling_impl): _StatsScaling(\n",
       "          (affine_rescaling): Identity()\n",
       "          (restrict_clamp_scaling): _RestrictClampValue(\n",
       "            (clamp_min_ste): Identity()\n",
       "            (restrict_value_impl): FloatRestrictValue()\n",
       "          )\n",
       "          (restrict_scaling_pre): Identity()\n",
       "        )\n",
       "      )\n",
       "      (int_scaling_impl): IntScaling()\n",
       "      (zero_point_impl): ZeroZeroPoint(\n",
       "        (zero_point): StatelessBuffer()\n",
       "      )\n",
       "      (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "        (bit_width): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bias_quant): BiasQuantProxyFromInjector(\n",
       "    (_zero_hw_sentinel): StatelessBuffer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.remove(net.conv0, 'weight')\n",
    "prune.remove(net.conv1, 'weight')\n",
    "prune.remove(net.conv2, 'weight')\n",
    "prune.remove(net.conv3, 'weight')\n",
    "prune.remove(net.conv4, 'weight')\n",
    "prune.remove(net.conv5, 'weight')\n",
    "prune.remove(net.conv6, 'weight')\n",
    "prune.remove(net.conv7, 'weight')\n",
    "prune.remove(net.fc1, 'weight')\n",
    "prune.remove(net.fc2, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), model_save_path_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
